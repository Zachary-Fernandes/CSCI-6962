{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Zachary-Fernandes/CSCI-6962/blob/main/fernazHW6v4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXZog3W-zdgW"
      },
      "source": [
        "# Homework 6\n",
        "#####Zachary Fernandes\n",
        "#####Projects in Machine Learning and AI"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Markov Decision Processes (MDP)\n",
        "\n",
        "As demonstrated by [this article](https://towardsdatascience.com/real-world-applications-of-markov-decision-process-mdp-a39685546026), we can formulate a game show as an MDP. One round in this game involves asking a participant a question. If they answer correctly, they earn money, but if they answer incorrectly, they lose all their earned money. Answering correctly also means the participant can either choose to continue to the next round or quit and keep all their money. As rounds increase in number, their rewards also grow, but they become more and more challenging. For example, our game could last fifteen rounds\n",
        "\n",
        "The state space would include each of the rounds along with an end game state.\n",
        "\n",
        "The action space would be the choices a player can make, those being playing the next round or quitting.\n",
        "\n",
        "The transition model would focus on what actions the player takes. If they play the next round, they may win or lose that round. Winning means moving to the next round with some probability _p_; the reward the player earns is how much is offered for the current round. If the player wins the last round, they earn the final prize and move to the end game state as no rounds remain. Losing means losing all their money and moving to the end game state with 1 - _p_ probability. If the player chooses to quit, they move to the end game state without any further reward and probability 1.\n",
        "\n",
        "The reward is how much money a player earns for a correct answer. This reward increases as the round number increases. Additionally, as the reward value and round number increase, the probability of answering correctly decreases."
      ],
      "metadata": {
        "id": "B670T7WxoolH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reinforcement Learning (RL) in Healthcare\n",
        "\n",
        "One issue with healthcare that can be more effectively solved by RL is creating and configuring dynamic treatment regimes (DTRs). A DTR is, as put by [CapeStart](https://www.capestart.com/resources/blog/reinforcement-learning-in-health-care-why-its-important-and-how-it-can-help/), a sequence of rules that determine healthcare decisions, such as types of treatment, dosages of drugs, and timing of appointments. These are tailored to a specific patient based on medical history and conditions. For an RL algorithm, clinical observations and patient assessments would be its input data, and treatment options would be its output. This is done to reach the patient's most desired environmental state.\n",
        "\n",
        "RL can help automate decision-making while treatment regimes proceed, design DTRs for chronic conditions, and improve critical care from intensive care data. One open-source approach to this problem is [pydtr](https://github.com/fullflu/pydtr). pydtr is a Python library that conducts DTRs, and it can select optimal treatments for specific patients with sklearn-based interfaces. It conducts these DTRs through a regression version of Iterative Q-Learning. It is possible to use sklearn-based models or statsmodels-based models - if one is using the sklearn model for a regression function and there are categorical independent variables, one must encode the categorical variables prior to using the model."
      ],
      "metadata": {
        "id": "0MpJA-hBopCC"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CfwU6Ho80N9i"
      },
      "source": [
        "### Tic-Tac-Toe\n",
        "\n",
        "This code creates an RL agent that competes against an irrational agent that chooses its moves at random. The intelligent agent uses tabular Q-learning to learn how to defeat the unintelligent agent.\n",
        "\n",
        "The evaluation metrics I will be using are the Player 1 win rate, the Player 2 win rate, and the draw rate. These are calculated after training and testing.\n",
        "\n",
        "Online Resources that Helped:\n",
        "\n",
        "[Framing Tic-Tac-Toe as a Reinforcement Learning Problem](https://levelup.gitconnected.com/framing-tic-tac-toe-as-a-reinforcement-learning-problem-eb76b6ece4de)\n",
        "\n",
        "[Setting up Tic-Tac-Toe for Reinforcement Learning in Python](https://levelup.gitconnected.com/setting-up-tic-tac-toe-for-reinforcement-learning-in-python-43e2f42cfce8)\n",
        "\n",
        "[Tabular Q-Learning Agent vs. Irrational Agent in the Game of Tic-Tac-Toe](https://levelup.gitconnected.com/tabular-q-learning-agent-vs-irrational-agent-in-the-game-of-tic-tac-toe-6de6c85f0c42)\n",
        "\n",
        "[Reinforcement Learning â€” Implement TicTacToe](https://towardsdatascience.com/reinforcement-learning-implement-tictactoe-189582bea542)\n",
        "\n",
        "This notebook features training an RL agent against an irrational agent that chooses actions at random. There is [another notebook](https://github.com/Zachary-Fernandes/CSCI-6962/blob/main/fernazHW6v5.ipynb), albeit less well-documented than this, that trains two Q-learning RL agents against each other, and a human can then challenge one of them."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Setup"
      ],
      "metadata": {
        "id": "D4K0Qtu8JXgd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MoRfsvI_CwMB",
        "outputId": "e2ad76f3-e270-42bf-9ae6-313f555ad5db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.7/dist-packages (0.8.1)\n",
            "Requirement already satisfied: scikit-learn>=0.24 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn) (1.0.2)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn) (1.7.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn) (1.2.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn) (1.21.6)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.24->imbalanced-learn) (3.1.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.7/dist-packages (0.10.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow-probability in /usr/local/lib/python3.7/dist-packages (0.17.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability) (4.4.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability) (1.3.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability) (1.21.6)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability) (1.5.0)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability) (0.1.7)\n",
            "Requirement already satisfied: gast>=0.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability) (0.4.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.7/dist-packages (2.9.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from imageio) (1.21.6)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from imageio) (7.1.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/tensorflow/docs\n",
            "  Cloning https://github.com/tensorflow/docs to /tmp/pip-req-build-kecabxzb\n",
            "  Running command git clone -q https://github.com/tensorflow/docs /tmp/pip-req-build-kecabxzb\n",
            "Requirement already satisfied: astor in /usr/local/lib/python3.7/dist-packages (from tensorflow-docs==0.0.0.dev0) (0.8.1)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from tensorflow-docs==0.0.0.dev0) (1.3.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-docs==0.0.0.dev0) (2.11.3)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.7/dist-packages (from tensorflow-docs==0.0.0.dev0) (5.7.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-docs==0.0.0.dev0) (3.19.6)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from tensorflow-docs==0.0.0.dev0) (6.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->tensorflow-docs==0.0.0.dev0) (2.0.1)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from nbformat->tensorflow-docs==0.0.0.dev0) (4.11.2)\n",
            "Requirement already satisfied: traitlets>=5.1 in /usr/local/lib/python3.7/dist-packages (from nbformat->tensorflow-docs==0.0.0.dev0) (5.1.1)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.7/dist-packages (from nbformat->tensorflow-docs==0.0.0.dev0) (4.3.3)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.7/dist-packages (from nbformat->tensorflow-docs==0.0.0.dev0) (2.16.2)\n",
            "Requirement already satisfied: importlib-metadata>=3.6 in /usr/local/lib/python3.7/dist-packages (from nbformat->tensorflow-docs==0.0.0.dev0) (4.13.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=3.6->nbformat->tensorflow-docs==0.0.0.dev0) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=3.6->nbformat->tensorflow-docs==0.0.0.dev0) (3.10.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat->tensorflow-docs==0.0.0.dev0) (22.1.0)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat->tensorflow-docs==0.0.0.dev0) (5.10.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat->tensorflow-docs==0.0.0.dev0) (0.19.2)\n",
            "Building wheels for collected packages: tensorflow-docs\n",
            "  Building wheel for tensorflow-docs (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tensorflow-docs: filename=tensorflow_docs-0.0.0.dev0-py3-none-any.whl size=184426 sha256=fb1a70397821ada45e1df253daf8ec0f2325bd4f6732ad5b573d06997180bcbb\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-6nukigur/wheels/cc/c4/d8/5341e93b6376c5c929c49469fce21155eb69cef1a4da4ce32c\n",
            "Successfully built tensorflow-docs\n",
            "Installing collected packages: tensorflow-docs\n",
            "Successfully installed tensorflow-docs-0.0.0.dev0\n"
          ]
        }
      ],
      "source": [
        "!pip install imbalanced-learn\n",
        "!pip install graphviz\n",
        "!pip install tensorflow-probability\n",
        "\n",
        "# to generate gifs\n",
        "!pip install imageio\n",
        "!pip install git+https://github.com/tensorflow/docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "aZCA63injOt2"
      },
      "outputs": [],
      "source": [
        "# import required libraries\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import random\n",
        "import graphviz\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import KFold \n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn import tree\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.preprocessing import Normalizer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn import utils\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import feature_column\n",
        "from tensorflow import keras\n",
        "import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import Dense, BatchNormalization\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "import IPython\n",
        "import IPython.display\n",
        "\n",
        "from gensim.test.utils import datapath, get_tmpfile\n",
        "from gensim.models import KeyedVectors\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec\n",
        "\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "import glob\n",
        "import imageio\n",
        "import PIL\n",
        "import tensorflow_probability as tfp\n",
        "import time\n",
        "import tensorflow_docs.vis.embed as embed\n",
        "\n",
        "from scipy import ndimage, misc\n",
        "import cv2"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Board Checking"
      ],
      "metadata": {
        "id": "pWaAfBLjJcyv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def check_over(x, printFlag = 0):\n",
        "    \"\"\"\n",
        "    Takes in the current board state x and a flag of whether or not to print\n",
        "    output, determines if the game is won by either player or if the game ends\n",
        "    in a draw.\n",
        "    Returns True if episode is over. \n",
        "    Returns False if episode is not over. \n",
        "    Also, returns a string '1' if Player 1 has won,\n",
        "    a string '2' if Player 2 has won, or\n",
        "    a string '3' if there is a draw. \n",
        "    \"\"\"\n",
        "    # Check if Player 1 won\n",
        "    # First check each row\n",
        "    # Then check each column\n",
        "    # Finally check each diagonal\n",
        "    if np.prod(x[0,:]) == 1 or np.prod(x[1,:]) == 1 or np.prod(x[2,:]) == 1 or \\\n",
        "        np.prod(x[:,0]) == 1 or np.prod(x[:,1]) == 1 or np.prod(x[:,2]) == 1 or \\\n",
        "        np.prod(np.diag(x)) == 1 or np.prod(np.diag(np.rot90(x))) == 1:\n",
        "        if printFlag == 1:\n",
        "            # Printing occurs during some testing\n",
        "            # and human matches\n",
        "            print(\"Player 1 won!\")\n",
        "        done = True\n",
        "        player_win = '1'\n",
        "        return done, player_win\n",
        "\n",
        "    # Check if Player 2 won\n",
        "    # First check each row\n",
        "    # Then check each column\n",
        "    # Finally check each diagonal\n",
        "    if np.sum(x[0,:]) == 6 or np.sum(x[1,:]) == 6 or np.sum(x[2,:]) == 6 or \\\n",
        "        np.sum(x[:,0]) == 6 or np.sum(x[:,1]) == 6 or np.sum(x[:,2]) == 6 or \\\n",
        "        np.sum(np.diag(x)) == 6 or np.sum(np.diag(np.rot90(x))) == 6:\n",
        "        if printFlag == 1:\n",
        "            print(\"Player 2 won!\")\n",
        "        done = True\n",
        "        player_win = '2'\n",
        "        return done, player_win\n",
        "\n",
        "    # Check if a draw occurred \n",
        "    if np.all(x):\n",
        "        if printFlag == 1:\n",
        "            print(\"Draw!\")\n",
        "        done = True\n",
        "        player_win = 'd'\n",
        "        return done, player_win\n",
        "    # Match is not over\n",
        "    return False, ' '"
      ],
      "metadata": {
        "id": "FhBZ9xrIXS8n"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### RL Parameter Setup"
      ],
      "metadata": {
        "id": "VQl7hfxgJ9FJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Player symbols - P1 = X, P2 = O\n",
        "symbols = {1: 'X', 2: 'O'}\n",
        "\n",
        "# Max number of state values per cell\n",
        "max_vals = 3\n",
        "# Max number of controls (actions)\n",
        "max_controls = 9\n",
        "# Create two Q-matrices\n",
        "# The first nine dimensions are reserved for states\n",
        "# The tenth dimension is for controls\n",
        "Qvals = np.zeros((max_vals, max_vals, max_vals, max_vals, max_vals,\n",
        "                  max_vals, max_vals, max_vals, max_vals, max_controls))"
      ],
      "metadata": {
        "id": "O_-5VMDIl3Ug"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Learning rate\n",
        "alpha = 0.9\n",
        "# Discount factor\n",
        "gamma = 0.99\n",
        "# Value that determines the probability the agent will\n",
        "# use a random control (exploration) or use its Q-table\n",
        "# for the best control (exploitation)\n",
        "# Epsilon = 1.0 means only exploring controls is done at the start, and as\n",
        "# it decays, the RL agent will begin exploiting more and more controls\n",
        "# This is a key part of the epsilon-greedy approach used in this program\n",
        "epsilon = 1.0\n",
        "\n",
        "# How long to train\n",
        "epochs = 100000\n",
        "\n",
        "# Track training metrics for Player 1 wins, Player 2 wins, and draws\n",
        "winP1 = np.zeros((epochs,))\n",
        "winP2 = np.zeros((epochs,))\n",
        "draw = np.zeros((epochs,))\n",
        "\n",
        "# 20 epsilon decay intervals\n",
        "decayStages = 20\n",
        "# How long each decay interval lasts\n",
        "stageLength = epochs // decayStages\n",
        "# How much we decay epsilon by for each stage\n",
        "decayFactor = 0.7"
      ],
      "metadata": {
        "id": "P7xBl2I-axoL"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Q-table: ', Qvals.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zB9TvPY1dLPS",
        "outputId": "473f3e69-acf1-46d0-8054-71c802c7a409"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q-table:  (3, 3, 3, 3, 3, 3, 3, 3, 3, 9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### RL Helper Functions"
      ],
      "metadata": {
        "id": "Ml_B8kFGJoba"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def control_train(x, epsilon, controls, Qvals):\n",
        "    \"\"\"\n",
        "    Take in current board position x, epsilon (probability for random control),\n",
        "    control space, and Q-table being learned.\n",
        "    Return eithers a random control (exploration) or the best possible control\n",
        "    according to the Q-table (exploitation) depending on the epsilon-greedy\n",
        "    policy.\n",
        "    \"\"\"\n",
        "    if np.random.random() >= epsilon:\n",
        "        # Exploitation time - choose best control according to Q-table\n",
        "        u = np.argmax(Qvals[x])\n",
        "\n",
        "        # First check if u is a possible control\n",
        "        if u not in controls:\n",
        "            # We cannot exploit - our fallback is to explore\n",
        "            new_control = np.random.choice(list(controls))\n",
        "            return new_control\n",
        "        else:\n",
        "            # We can exploit with u\n",
        "            return u\n",
        "    else:\n",
        "        # Exploration time - choose random control\n",
        "        u = np.random.choice(list(controls))\n",
        "        return u"
      ],
      "metadata": {
        "id": "X_g4su5edi1x"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def control_test(x, controls, Qvals):\n",
        "    \"\"\"\n",
        "    Take in current board position x, control space, and trained Q-table.\n",
        "    Return an intelligent control determined by the knowledge stored in the\n",
        "    Q-table (exploitation).\n",
        "    \"\"\"\n",
        "    u = np.argmax(Qvals[x])\n",
        "    \n",
        "    # First check if u is a possible control\n",
        "    if u not in controls:\n",
        "        # We cannot exploit - our fallback is to explore\n",
        "        new_control = np.random.choice(list(controls))\n",
        "        return new_control\n",
        "    else:\n",
        "        # We can exploit with u\n",
        "        return u"
      ],
      "metadata": {
        "id": "Dy4bMKz5esiY"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decay_epsilon(episode, epsilon, stageLength, decayFactor):\n",
        "    # Decays epsilon at a new period\n",
        "    if episode % stageLength == 0 and episode != 0:\n",
        "        epsilon *= decayFactor\n",
        "        print(f'epsilon = {epsilon}')\n",
        "    return epsilon"
      ],
      "metadata": {
        "id": "DXs39u1wesf4"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def reset():\n",
        "    # Resets the control space at each epoch's beginning\n",
        "    controls = set()\n",
        "    for i in range(9):\n",
        "        controls.add(i)\n",
        "    return controls"
      ],
      "metadata": {
        "id": "A6J98pVSfdU5"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def batch_update(statesRL, controlsRL, r, nextStatesRL, gamma, alpha, Qvals):\n",
        "    \"\"\"\n",
        "    Agent learns from the state-control moves that led to a specific outcome,\n",
        "    with reward or penalty r being applied. This implements the Q-table update\n",
        "    rule. Updating the Q-table for the state-control pair on the game's end led\n",
        "    to slow learning, which is why we employ batch learning across all the\n",
        "    state-control actions.\n",
        "    \"\"\"\n",
        "    # Employ batch learning on all state-control actions\n",
        "    for i, boardVals in enumerate(statesRL):\n",
        "        u = controlsRL[i]\n",
        "        boardValsNext = nextStatesRL[i]\n",
        "        td_target = r + gamma * np.max(Qvals[boardValsNext])\n",
        "        td_error = td_target - Qvals[boardVals][u]\n",
        "        Qvals[boardVals][u] += alpha * td_error\n",
        "    return Qvals"
      ],
      "metadata": {
        "id": "ihjt1-K2frwq"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training"
      ],
      "metadata": {
        "id": "Wlyyn2Z3JjjL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Training Cell\n",
        "\"\"\"\n",
        "\n",
        "# Reward for winning a game\n",
        "reward = 10\n",
        "\n",
        "# Loop over each epoch\n",
        "for epoch in range(epochs):\n",
        "    # Print which epoch this is to see if it hangs\n",
        "    if epoch % 1000 == 0:\n",
        "        print(epoch)\n",
        "    # Hold agent's states and controls to easily update the Q-table\n",
        "    statesRL = []\n",
        "    controlsRL = []\n",
        "    nextStatesRL = []\n",
        "\n",
        "    done = False\n",
        "    controls = reset()\n",
        "\n",
        "    # Starting board state - empty\n",
        "    boardVals = np.zeros((3,3))\n",
        "    turn = 0\n",
        "    # Visual representation of the board\n",
        "    board = np.array([[' ', ' ', ' '],\n",
        "                      [' ', ' ', ' '],\n",
        "                      [' ', ' ', ' ']])\n",
        "    # Decay epsilon as needed\n",
        "    epsilon = decay_epsilon(epoch, epsilon, stageLength, decayFactor)\n",
        "\n",
        "    # Game loop\n",
        "    while done != True:\n",
        "        # Determine which player's turn it is\n",
        "        if turn % 2 == 0:\n",
        "            # Player 1's turn\n",
        "            # Epsilon-greedy training policy\n",
        "            u = control_train(tuple(boardVals.reshape(-1,).astype(int)),\n",
        "                              epsilon, controls, Qvals)\n",
        "            statesRL.append(tuple(boardVals.reshape(-1,).astype(int)))\n",
        "            controlsRL.append(u)\n",
        "            # Using a move, remove it\n",
        "            controls.remove(u)\n",
        "            # Update board state\n",
        "            boardValsNext = boardVals.reshape(-1,)\n",
        "            boardValsNext[u] = 1\n",
        "            boardValsNext = boardValsNext.reshape(3,3)\n",
        "            nextStatesRL.append(tuple(boardValsNext.reshape(-1,).astype(int)))\n",
        "            # Update visual representation\n",
        "            board = board.reshape(-1,)\n",
        "            board[u] = symbols[1]\n",
        "            board = board.reshape(3,3)\n",
        "\n",
        "            # Determine if the game finished\n",
        "            done, winner = check_over(boardVals)\n",
        "            if done == True:\n",
        "                if winner == '1':\n",
        "                    # Player 1 (agent) learns from moves responsible for winning\n",
        "                    Qvals = batch_update(statesRL,\n",
        "                                         controlsRL,\n",
        "                                         reward,\n",
        "                                         nextStatesRL,\n",
        "                                         gamma,\n",
        "                                         alpha,\n",
        "                                         Qvals)\n",
        "                    winP1[epoch] = 1\n",
        "                elif winner == 'd':\n",
        "                    # Game draw, neither player wins\n",
        "                    draw[epoch] = 1\n",
        "            # Update board state, move to next turn\n",
        "            boardVals = boardValsNext\n",
        "            turn += 1\n",
        "        else:\n",
        "            # Player 2's turn\n",
        "            u = np.random.choice(list(controls))\n",
        "            # Using a move, remove it\n",
        "            controls.remove(u)\n",
        "            # Update board state\n",
        "            boardVals = boardVals.reshape(-1,)\n",
        "            boardVals[u] = 2\n",
        "            boardVals = boardVals.reshape(3,3)\n",
        "            # Update visual representation\n",
        "            board = board.reshape(-1,)\n",
        "            board[u] = symbols[2]\n",
        "            board = board.reshape(3,3)\n",
        "            turn += 1\n",
        "\n",
        "            # Determine if the game finished\n",
        "            done, winner = check_over(boardVals)\n",
        "            if done == True:\n",
        "                if winner == '2':\n",
        "                    # Player 2 wins\n",
        "                    winP2[epoch] = 1\n",
        "                elif winner == 'd':\n",
        "                    # Game draw, neither player wins\n",
        "                    draw[epoch] = 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fgqlKDayhiIU",
        "outputId": "2fb75953-e5a3-4d4b-c109-50693b65f701"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "1000\n",
            "2000\n",
            "3000\n",
            "4000\n",
            "5000\n",
            "epsilon = 0.7\n",
            "6000\n",
            "7000\n",
            "8000\n",
            "9000\n",
            "10000\n",
            "epsilon = 0.48999999999999994\n",
            "11000\n",
            "12000\n",
            "13000\n",
            "14000\n",
            "15000\n",
            "epsilon = 0.3429999999999999\n",
            "16000\n",
            "17000\n",
            "18000\n",
            "19000\n",
            "20000\n",
            "epsilon = 0.24009999999999992\n",
            "21000\n",
            "22000\n",
            "23000\n",
            "24000\n",
            "25000\n",
            "epsilon = 0.16806999999999994\n",
            "26000\n",
            "27000\n",
            "28000\n",
            "29000\n",
            "30000\n",
            "epsilon = 0.11764899999999995\n",
            "31000\n",
            "32000\n",
            "33000\n",
            "34000\n",
            "35000\n",
            "epsilon = 0.08235429999999996\n",
            "36000\n",
            "37000\n",
            "38000\n",
            "39000\n",
            "40000\n",
            "epsilon = 0.05764800999999997\n",
            "41000\n",
            "42000\n",
            "43000\n",
            "44000\n",
            "45000\n",
            "epsilon = 0.04035360699999998\n",
            "46000\n",
            "47000\n",
            "48000\n",
            "49000\n",
            "50000\n",
            "epsilon = 0.028247524899999984\n",
            "51000\n",
            "52000\n",
            "53000\n",
            "54000\n",
            "55000\n",
            "epsilon = 0.019773267429999988\n",
            "56000\n",
            "57000\n",
            "58000\n",
            "59000\n",
            "60000\n",
            "epsilon = 0.01384128720099999\n",
            "61000\n",
            "62000\n",
            "63000\n",
            "64000\n",
            "65000\n",
            "epsilon = 0.009688901040699992\n",
            "66000\n",
            "67000\n",
            "68000\n",
            "69000\n",
            "70000\n",
            "epsilon = 0.006782230728489994\n",
            "71000\n",
            "72000\n",
            "73000\n",
            "74000\n",
            "75000\n",
            "epsilon = 0.004747561509942996\n",
            "76000\n",
            "77000\n",
            "78000\n",
            "79000\n",
            "80000\n",
            "epsilon = 0.003323293056960097\n",
            "81000\n",
            "82000\n",
            "83000\n",
            "84000\n",
            "85000\n",
            "epsilon = 0.002326305139872068\n",
            "86000\n",
            "87000\n",
            "88000\n",
            "89000\n",
            "90000\n",
            "epsilon = 0.0016284135979104473\n",
            "91000\n",
            "92000\n",
            "93000\n",
            "94000\n",
            "95000\n",
            "epsilon = 0.001139889518537313\n",
            "96000\n",
            "97000\n",
            "98000\n",
            "99000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute Player 1 win rate, Player 2 win rate, and draw rate averages\n",
        "winTotP1 = np.cumsum(winP1)\n",
        "winTotP2 = np.cumsum(winP2)\n",
        "drawTot = np.cumsum(draw)\n",
        "\n",
        "winRateP1 = np.zeros(epochs,)\n",
        "winRateP2 = np.zeros(epochs,)\n",
        "drawRate = np.zeros(epochs,)\n",
        "\n",
        "for e in range(epochs):\n",
        "    winRateP1[e] = winTotP1[e] / (e + 1)\n",
        "    winRateP2[e] = winTotP2[e] / (e + 1)\n",
        "    drawRate[e] = drawTot[e] / (e + 1)\n",
        "\n",
        "# Plot these rates over epochs\n",
        "plt.figure()\n",
        "plt.plot(np.arange(epochs), winRateP1, 'g-.', label = 'P1 Win Rate')\n",
        "plt.plot(np.arange(epochs), winRateP2, 'r:.', label = 'P2 Win Rate')\n",
        "plt.plot(np.arange(epochs), drawRate, 'k-', label = 'Draw Rate')\n",
        "plt.xlabel('Epochs')\n",
        "plt.legend()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "U6TbyutUk_h3",
        "outputId": "e3236dad-6fd2-4622-ea5e-4ca3c3c8c363"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f3197eaffd0>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# From the above, we can see our RL agent is learning to beat the irrational\n",
        "# agent, with P1's win rate increasing while P2's win rate and the draw rate\n",
        "# decrease. This implies the RL agent is learning an effective policy.\n",
        "winRateP1[-1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KTAibnh6pruo",
        "outputId": "69d94822-f332-4849-bab4-cad1a8815a29"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.84269"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "winRateP2[-1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0XZNsR0cpwAF",
        "outputId": "05659285-2bb2-4f2e-83a5-0c1917798b25"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.11153"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drawRate[-1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W4Ghus8SpxnZ",
        "outputId": "c9f618f5-3699-405e-c02e-134159998348"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.04578"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sanity check for these values to add up\n",
        "winRateP1[-1] + winRateP2[-1] + drawRate[-1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5JlxuaPSslRa",
        "outputId": "4673cc82-83c1-4528-e171-ff3ca08b36f8"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Testing"
      ],
      "metadata": {
        "id": "6zk-NlhpKKOh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Testing Cell\n",
        "\"\"\"\n",
        "\n",
        "def test(Qvals, epochs):\n",
        "    \"\"\"\n",
        "    Uses the learned Q-table to its fullest, testing against the irrational\n",
        "    agent for a number of epochs.\n",
        "    \"\"\"\n",
        "    # Track metrics for evaluation - Player 1 wins, Player 2 wins, and draws\n",
        "    winP1 = 0\n",
        "    winP2 = 0\n",
        "    draw = 0\n",
        "    # Symbols: P1 = X, P2 = O\n",
        "    symbols = {1: 'X', 2: 'O'}\n",
        "\n",
        "    # Run for epochs\n",
        "    for epoch in range(epochs):\n",
        "        done = False\n",
        "        controls = reset()\n",
        "\n",
        "        # Starting board state - empty\n",
        "        boardVals = np.zeros((3,3))\n",
        "        turn = 0\n",
        "        # Visual representation of the board\n",
        "        board = np.array([[' ', ' ', ' '],\n",
        "                          [' ', ' ', ' '],\n",
        "                          [' ', ' ', ' ']])\n",
        "\n",
        "        # Demonstrate five test runs\n",
        "        if epoch % (epochs // 5) == 0:\n",
        "            print(f'Episode {epoch}, turn = {turn}, \\n', board)\n",
        "\n",
        "        # Game loop\n",
        "        while done != True:\n",
        "            # Determine which player's turn it is\n",
        "            if turn % 2 == 0:\n",
        "                # Player 1's turn\n",
        "                # Epsilon-greedy policy\n",
        "                u = control_test(tuple(boardVals.reshape(-1,).astype(int)),\n",
        "                                 controls, Qvals)\n",
        "                # Using a move, remove it\n",
        "                controls.remove(u)\n",
        "                # Update board state\n",
        "                boardValsNext = boardVals.reshape(-1,)\n",
        "                boardValsNext[u] = 1\n",
        "                boardValsNext = boardValsNext.reshape(3,3)\n",
        "                # Update visual representation\n",
        "                board = board.reshape(-1,)\n",
        "                board[u] = symbols[1]\n",
        "                board = board.reshape(3,3)\n",
        "\n",
        "                # Update board state, move to next turn\n",
        "                boardVals = boardValsNext\n",
        "                turn += 1\n",
        "\n",
        "                # Demonstrate five test runs\n",
        "                # Determine if the game finished\n",
        "                if epoch % (epochs // 5) == 0:\n",
        "                    print(f'Episode {epoch}, turn = {turn}, \\n', board)\n",
        "                    done, winner = check_over(boardVals, 1)\n",
        "                else:\n",
        "                    done, winner = check_over(boardVals)\n",
        "                \n",
        "                if done:\n",
        "                    if winner == '1':\n",
        "                        # Player 1 win\n",
        "                        winP1 += 1\n",
        "                    elif winner == '2':\n",
        "                        # Player 2 win\n",
        "                        winP2 += 1\n",
        "                    elif winner == 'd':\n",
        "                        # Draw, neither win\n",
        "                        draw += 1\n",
        "                \n",
        "            else:\n",
        "                # Player 2's turn\n",
        "                u = np.random.choice(list(controls))\n",
        "                # Using a move, remove it\n",
        "                controls.remove(u)\n",
        "                # Update board state\n",
        "                boardVals = boardVals.reshape(-1,)\n",
        "                boardVals[u] = 2\n",
        "                boardVals = boardVals.reshape(3,3)\n",
        "                # Update visual representation\n",
        "                board = board.reshape(-1,)\n",
        "                board[u] = symbols[2]\n",
        "                board = board.reshape(3,3)\n",
        "                turn += 1\n",
        "\n",
        "                # Demonstrate five test runs\n",
        "                # Determine if the game finished\n",
        "                if epoch % (epochs // 5) == 0:\n",
        "                    print(f'Episode {epoch}, turn = {turn}, \\n', board)\n",
        "                    done, winner = check_over(boardVals, 1)\n",
        "                else:\n",
        "                    done, winner = check_over(boardVals)\n",
        "\n",
        "                if done:\n",
        "                    if winner == '1':\n",
        "                        # Player 1 win\n",
        "                        winP1 += 1\n",
        "                    elif winner == '2':\n",
        "                        # Player 2 win\n",
        "                        winP2 += 1\n",
        "                    elif winner == 'd':\n",
        "                        # Draw, neither win\n",
        "                        draw += 1\n",
        "\n",
        "    # Compute Player 1 win, Player 2 win, and draw rates\n",
        "    winRateP1 = winP1 / epochs\n",
        "    winRateP2 = winP2 / epochs\n",
        "    drawRate = draw / epochs\n",
        "\n",
        "    return winRateP1, winRateP2, drawRate"
      ],
      "metadata": {
        "id": "AEJ07FFqs0iV"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testEpochs = 10000\n",
        "winRateP1, winRateP2, drawRate = test(Qvals, testEpochs)"
      ],
      "metadata": {
        "id": "WI1Iv0oAAoL3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "937b1483-fd96-4fe6-d928-c6e60ccb8572"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 0, turn = 0, \n",
            " [[' ' ' ' ' ']\n",
            " [' ' ' ' ' ']\n",
            " [' ' ' ' ' ']]\n",
            "Episode 0, turn = 0, \n",
            " [['X' ' ' ' ']\n",
            " [' ' ' ' ' ']\n",
            " [' ' ' ' ' ']]\n",
            "Episode 0, turn = 2, \n",
            " [['X' ' ' ' ']\n",
            " ['O' ' ' ' ']\n",
            " [' ' ' ' ' ']]\n",
            "Episode 0, turn = 2, \n",
            " [['X' 'X' ' ']\n",
            " ['O' ' ' ' ']\n",
            " [' ' ' ' ' ']]\n",
            "Episode 0, turn = 4, \n",
            " [['X' 'X' ' ']\n",
            " ['O' ' ' ' ']\n",
            " [' ' 'O' ' ']]\n",
            "Episode 0, turn = 4, \n",
            " [['X' 'X' 'X']\n",
            " ['O' ' ' ' ']\n",
            " [' ' 'O' ' ']]\n",
            "Player 1 won!\n",
            "Episode 2000, turn = 0, \n",
            " [[' ' ' ' ' ']\n",
            " [' ' ' ' ' ']\n",
            " [' ' ' ' ' ']]\n",
            "Episode 2000, turn = 0, \n",
            " [['X' ' ' ' ']\n",
            " [' ' ' ' ' ']\n",
            " [' ' ' ' ' ']]\n",
            "Episode 2000, turn = 2, \n",
            " [['X' ' ' ' ']\n",
            " [' ' 'O' ' ']\n",
            " [' ' ' ' ' ']]\n",
            "Episode 2000, turn = 2, \n",
            " [['X' 'X' ' ']\n",
            " [' ' 'O' ' ']\n",
            " [' ' ' ' ' ']]\n",
            "Episode 2000, turn = 4, \n",
            " [['X' 'X' ' ']\n",
            " [' ' 'O' ' ']\n",
            " ['O' ' ' ' ']]\n",
            "Episode 2000, turn = 4, \n",
            " [['X' 'X' 'X']\n",
            " [' ' 'O' ' ']\n",
            " ['O' ' ' ' ']]\n",
            "Player 1 won!\n",
            "Episode 4000, turn = 0, \n",
            " [[' ' ' ' ' ']\n",
            " [' ' ' ' ' ']\n",
            " [' ' ' ' ' ']]\n",
            "Episode 4000, turn = 0, \n",
            " [['X' ' ' ' ']\n",
            " [' ' ' ' ' ']\n",
            " [' ' ' ' ' ']]\n",
            "Episode 4000, turn = 2, \n",
            " [['X' ' ' ' ']\n",
            " [' ' ' ' ' ']\n",
            " ['O' ' ' ' ']]\n",
            "Episode 4000, turn = 2, \n",
            " [['X' 'X' ' ']\n",
            " [' ' ' ' ' ']\n",
            " ['O' ' ' ' ']]\n",
            "Episode 4000, turn = 4, \n",
            " [['X' 'X' ' ']\n",
            " [' ' ' ' ' ']\n",
            " ['O' ' ' 'O']]\n",
            "Episode 4000, turn = 4, \n",
            " [['X' 'X' 'X']\n",
            " [' ' ' ' ' ']\n",
            " ['O' ' ' 'O']]\n",
            "Player 1 won!\n",
            "Episode 6000, turn = 0, \n",
            " [[' ' ' ' ' ']\n",
            " [' ' ' ' ' ']\n",
            " [' ' ' ' ' ']]\n",
            "Episode 6000, turn = 0, \n",
            " [['X' ' ' ' ']\n",
            " [' ' ' ' ' ']\n",
            " [' ' ' ' ' ']]\n",
            "Episode 6000, turn = 2, \n",
            " [['X' ' ' ' ']\n",
            " [' ' ' ' 'O']\n",
            " [' ' ' ' ' ']]\n",
            "Episode 6000, turn = 2, \n",
            " [['X' 'X' ' ']\n",
            " [' ' ' ' 'O']\n",
            " [' ' ' ' ' ']]\n",
            "Episode 6000, turn = 4, \n",
            " [['X' 'X' 'O']\n",
            " [' ' ' ' 'O']\n",
            " [' ' ' ' ' ']]\n",
            "Episode 6000, turn = 4, \n",
            " [['X' 'X' 'O']\n",
            " ['X' ' ' 'O']\n",
            " [' ' ' ' ' ']]\n",
            "Episode 6000, turn = 6, \n",
            " [['X' 'X' 'O']\n",
            " ['X' ' ' 'O']\n",
            " [' ' 'O' ' ']]\n",
            "Episode 6000, turn = 6, \n",
            " [['X' 'X' 'O']\n",
            " ['X' ' ' 'O']\n",
            " ['X' 'O' ' ']]\n",
            "Player 1 won!\n",
            "Episode 8000, turn = 0, \n",
            " [[' ' ' ' ' ']\n",
            " [' ' ' ' ' ']\n",
            " [' ' ' ' ' ']]\n",
            "Episode 8000, turn = 0, \n",
            " [['X' ' ' ' ']\n",
            " [' ' ' ' ' ']\n",
            " [' ' ' ' ' ']]\n",
            "Episode 8000, turn = 2, \n",
            " [['X' ' ' ' ']\n",
            " [' ' ' ' ' ']\n",
            " [' ' ' ' 'O']]\n",
            "Episode 8000, turn = 2, \n",
            " [['X' 'X' ' ']\n",
            " [' ' ' ' ' ']\n",
            " [' ' ' ' 'O']]\n",
            "Episode 8000, turn = 4, \n",
            " [['X' 'X' ' ']\n",
            " [' ' 'O' ' ']\n",
            " [' ' ' ' 'O']]\n",
            "Episode 8000, turn = 4, \n",
            " [['X' 'X' 'X']\n",
            " [' ' 'O' ' ']\n",
            " [' ' ' ' 'O']]\n",
            "Player 1 won!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# These test results corroborate how well this policy performs\n",
        "winRateP1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nz7-KqqvA4tL",
        "outputId": "2ab45b11-a069-4128-8cf1-7ca54a1f4590"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8989"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "winRateP2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9m3pClSaA5xO",
        "outputId": "3da01726-c612-40c7-eba9-9c9829272404"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0709"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drawRate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dPOPVwaRA6vk",
        "outputId": "94188955-9cdd-4822-865f-b7c46d569f4e"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0302"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sanity check for these values to add up\n",
        "winRateP1 + winRateP2 + drawRate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E0c1VGSXAx5S",
        "outputId": "d9a9abf6-694e-4892-d368-6c4344135bc9"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### AI versus Human"
      ],
      "metadata": {
        "id": "7miXZpkAKOEO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Pit the RL agent (Player 1) against a human (Player 2)\n",
        "\"\"\"\n",
        "\n",
        "def testHuman(Qvals):\n",
        "    # Keep track of evaluation metrics\n",
        "    winP1 = 0\n",
        "    winP2 = 0\n",
        "    draw = 0\n",
        "    # Symbols: P1 = X, P2 = O\n",
        "    symbols = {1: 'X', 2: 'O'}\n",
        "    # Count the rounds (replacement for epochs)\n",
        "    round = 0\n",
        "\n",
        "    # Human decides whether to keep playing or end the session.\n",
        "    while int(input('Play (input 1) or exit (input 0)? ')) != 0:\n",
        "        round += 1\n",
        "        print('Round', round)\n",
        "        done = False\n",
        "        controls = reset()\n",
        "\n",
        "        # Starting board state - empty\n",
        "        boardVals = np.zeros((3,3))\n",
        "        turn = 0\n",
        "        # Visual representation of the board\n",
        "        board = np.array([[' ', ' ', ' '],\n",
        "                          [' ', ' ', ' '],\n",
        "                          [' ', ' ', ' ']])\n",
        "\n",
        "        print(f'Turn = {turn}, \\n', board)\n",
        "\n",
        "        while done != True:\n",
        "            # Determine which player's turn it is\n",
        "            if turn % 2 == 0:\n",
        "                # Player 1's turn - AI\n",
        "                u = control_test(tuple(boardVals.reshape(-1,).astype(int)),\n",
        "                                 controls, Qvals)\n",
        "                # Using a move, remove it\n",
        "                controls.remove(u)\n",
        "                # Update board state\n",
        "                boardValsNext = boardVals.reshape(-1,)\n",
        "                boardValsNext[u] = 1\n",
        "                boardValsNext = boardValsNext.reshape(3,3)\n",
        "                # Update visual representation\n",
        "                board = board.reshape(-1,)\n",
        "                board[u] = symbols[1]\n",
        "                board = board.reshape(3,3)\n",
        "\n",
        "                # Update board state, move to next turn \n",
        "                boardVals = boardValsNext\n",
        "                turn += 1\n",
        "                \n",
        "                print(f'Turn = {turn}, \\n', board)\n",
        "\n",
        "                # Determine if the game finished\n",
        "                done, winner = check_over(boardVals, 1)\n",
        "                if done:\n",
        "                    if winner == '1':\n",
        "                        # AI wins\n",
        "                        winP1 += 1\n",
        "                    elif winner == '2':\n",
        "                        # Human wins\n",
        "                        winP2 += 1\n",
        "                    elif winner == 'd':\n",
        "                        # Draw, neither win\n",
        "                        draw += 1\n",
        "\n",
        "            else:\n",
        "                # Player 2's turn - human\n",
        "                # Must choose which square:\n",
        "                #   1 2 3\n",
        "                #   4 5 6\n",
        "                #   7 8 9\n",
        "                u = int(input('Enter a square to place O (1-9): ')) - 1\n",
        "                # Using a move, remove it\n",
        "                controls.remove(u)\n",
        "                # Update board state\n",
        "                boardVals = boardVals.reshape(-1,)\n",
        "                boardVals[u] = 2\n",
        "                boardVals = boardVals.reshape(3,3)\n",
        "                # Update visual representation\n",
        "                board = board.reshape(-1,)\n",
        "                board[u] = symbols[2]\n",
        "                board = board.reshape(3,3)\n",
        "                turn += 1\n",
        "\n",
        "                print(f'Turn = {turn}, \\n', board)\n",
        "\n",
        "                # Determine if the game finished\n",
        "                done, winner = check_over(boardVals, 1)\n",
        "                if done:\n",
        "                    if winner == '1':\n",
        "                        # AI win\n",
        "                        winP1 += 1\n",
        "                    elif winner == '2':\n",
        "                        # Human win\n",
        "                        winP2 += 1\n",
        "                    elif winner == 'd':\n",
        "                        # Draw, neither win\n",
        "                        draw += 1\n",
        "    # Human ended session, return metrics\n",
        "    print('Game Over')\n",
        "    return winP1, winP2, draw, round"
      ],
      "metadata": {
        "id": "S952QhfVsIuI"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this cell (after the others)\n",
        "# to face the AI yourself\n",
        "print('Welcome to Tic-Tac-Toe! AI is P1, you are P2.')\n",
        "winP1, winP2, draw, rounds = testHuman(Qvals)\n",
        "\n",
        "if rounds == 0:\n",
        "    print('No games played; no stats reported.')\n",
        "else:\n",
        "    winRateP1 = winP1 / rounds\n",
        "    winRateP2 = winP2 / rounds\n",
        "    drawRate = draw / rounds\n",
        "    print('Rounds:       ', rounds)\n",
        "    print('AI Wins:      ', winP1)\n",
        "    print('AI Win Rate:  ', winRateP1)\n",
        "    print('Your Wins:    ', winP2)\n",
        "    print('Your Win Rate:', winRateP2)\n",
        "    print('Draws:        ', draw)\n",
        "    print('Draw Rate:    ', drawRate)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lk5VhJKu18J0",
        "outputId": "5556f849-440e-41fe-c524-fb32c41da26e"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Welcome to Tic-Tac-Toe! AI is P1, you are P2.\n",
            "Play (input 1) or exit (input 0)? 1\n",
            "Round 1\n",
            "Turn = 0, \n",
            " [[' ' ' ' ' ']\n",
            " [' ' ' ' ' ']\n",
            " [' ' ' ' ' ']]\n",
            "Turn = 0, \n",
            " [['X' ' ' ' ']\n",
            " [' ' ' ' ' ']\n",
            " [' ' ' ' ' ']]\n",
            "Enter a square to place O (1-9): 3\n",
            "Turn = 2, \n",
            " [['X' ' ' 'O']\n",
            " [' ' ' ' ' ']\n",
            " [' ' ' ' ' ']]\n",
            "Turn = 2, \n",
            " [['X' 'X' 'O']\n",
            " [' ' ' ' ' ']\n",
            " [' ' ' ' ' ']]\n",
            "Enter a square to place O (1-9): 4\n",
            "Turn = 4, \n",
            " [['X' 'X' 'O']\n",
            " ['O' ' ' ' ']\n",
            " [' ' ' ' ' ']]\n",
            "Turn = 4, \n",
            " [['X' 'X' 'O']\n",
            " ['O' 'X' ' ']\n",
            " [' ' ' ' ' ']]\n",
            "Enter a square to place O (1-9): 8\n",
            "Turn = 6, \n",
            " [['X' 'X' 'O']\n",
            " ['O' 'X' ' ']\n",
            " [' ' 'O' ' ']]\n",
            "Turn = 6, \n",
            " [['X' 'X' 'O']\n",
            " ['O' 'X' 'X']\n",
            " [' ' 'O' ' ']]\n",
            "Enter a square to place O (1-9): 7\n",
            "Turn = 8, \n",
            " [['X' 'X' 'O']\n",
            " ['O' 'X' 'X']\n",
            " ['O' 'O' ' ']]\n",
            "Turn = 8, \n",
            " [['X' 'X' 'O']\n",
            " ['O' 'X' 'X']\n",
            " ['O' 'O' 'X']]\n",
            "Player 1 won!\n",
            "Play (input 1) or exit (input 0)? 1\n",
            "Round 2\n",
            "Turn = 0, \n",
            " [[' ' ' ' ' ']\n",
            " [' ' ' ' ' ']\n",
            " [' ' ' ' ' ']]\n",
            "Turn = 0, \n",
            " [['X' ' ' ' ']\n",
            " [' ' ' ' ' ']\n",
            " [' ' ' ' ' ']]\n",
            "Enter a square to place O (1-9): 5\n",
            "Turn = 2, \n",
            " [['X' ' ' ' ']\n",
            " [' ' 'O' ' ']\n",
            " [' ' ' ' ' ']]\n",
            "Turn = 2, \n",
            " [['X' 'X' ' ']\n",
            " [' ' 'O' ' ']\n",
            " [' ' ' ' ' ']]\n",
            "Enter a square to place O (1-9): 3\n",
            "Turn = 4, \n",
            " [['X' 'X' 'O']\n",
            " [' ' 'O' ' ']\n",
            " [' ' ' ' ' ']]\n",
            "Turn = 4, \n",
            " [['X' 'X' 'O']\n",
            " ['X' 'O' ' ']\n",
            " [' ' ' ' ' ']]\n",
            "Enter a square to place O (1-9): 7\n",
            "Turn = 6, \n",
            " [['X' 'X' 'O']\n",
            " ['X' 'O' ' ']\n",
            " ['O' ' ' ' ']]\n",
            "Player 2 won!\n",
            "Play (input 1) or exit (input 0)? 1\n",
            "Round 3\n",
            "Turn = 0, \n",
            " [[' ' ' ' ' ']\n",
            " [' ' ' ' ' ']\n",
            " [' ' ' ' ' ']]\n",
            "Turn = 0, \n",
            " [['X' ' ' ' ']\n",
            " [' ' ' ' ' ']\n",
            " [' ' ' ' ' ']]\n",
            "Enter a square to place O (1-9): 4\n",
            "Turn = 2, \n",
            " [['X' ' ' ' ']\n",
            " ['O' ' ' ' ']\n",
            " [' ' ' ' ' ']]\n",
            "Turn = 2, \n",
            " [['X' 'X' ' ']\n",
            " ['O' ' ' ' ']\n",
            " [' ' ' ' ' ']]\n",
            "Enter a square to place O (1-9): 3\n",
            "Turn = 4, \n",
            " [['X' 'X' 'O']\n",
            " ['O' ' ' ' ']\n",
            " [' ' ' ' ' ']]\n",
            "Turn = 4, \n",
            " [['X' 'X' 'O']\n",
            " ['O' 'X' ' ']\n",
            " [' ' ' ' ' ']]\n",
            "Enter a square to place O (1-9): 9\n",
            "Turn = 6, \n",
            " [['X' 'X' 'O']\n",
            " ['O' 'X' ' ']\n",
            " [' ' ' ' 'O']]\n",
            "Turn = 6, \n",
            " [['X' 'X' 'O']\n",
            " ['O' 'X' 'X']\n",
            " [' ' ' ' 'O']]\n",
            "Enter a square to place O (1-9): 8\n",
            "Turn = 8, \n",
            " [['X' 'X' 'O']\n",
            " ['O' 'X' 'X']\n",
            " [' ' 'O' 'O']]\n",
            "Turn = 8, \n",
            " [['X' 'X' 'O']\n",
            " ['O' 'X' 'X']\n",
            " ['X' 'O' 'O']]\n",
            "Draw!\n",
            "Play (input 1) or exit (input 0)? 0\n",
            "Game Over\n",
            "Rounds:        3\n",
            "AI Wins:       1\n",
            "AI Win Rate:   0.3333333333333333\n",
            "Your Wins:     1\n",
            "Your Win Rate: 0.3333333333333333\n",
            "Draws:         1\n",
            "Draw Rate:     0.3333333333333333\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}